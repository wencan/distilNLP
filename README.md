# distilNLP
æœ¬é¡¹ç›®çš„ç›®çš„æ˜¯æä¾›ä¸€å¥—å¼€ç®±å³ç”¨ã€ä½ä¾èµ–ã€é«˜æ€§èƒ½ã€è§£å†³å®é™…é—®é¢˜èƒ½å–å¾—è‰¯å¥½æ•ˆæœçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·ç®±ã€‚  
ä¸åŒäºå…¶å®ƒNLPé¡¹ç›®é‡‡ç”¨ç»å…¸æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæˆ–è€…åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å»å¾®è°ƒï¼Œæœ¬é¡¹ç›®é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»å¤´è®¾è®¡å°å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ–åŸºäºé«˜åº¦å‹ç¼©çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥æœŸå–å¾—è¾ƒé«˜æ¨ç†å‡†ç¡®ç‡å’Œè¾ƒä½èµ„æºæ¶ˆè€—ã€‚  
ç°é˜¶æ®µï¼Œæœ¬é¡¹ç›®ä»…é™å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

The purpose of this project is to provide a set of out of the box, low-dependency, high-performance natural language processing toolkits that can achieve good results in solving real-world problems.  
Unlike other NLP projects that use classical machine learning techniques or fine-tune based on large-scale pre-trained models, this project uses deep learning techniques to design a series of small language models from scratch, or based on highly compressed pre-trained language models, in order to achieve higher inference accuracy and lower resource consumption.
At this stage, this project is only for learning and research.

# Usage
ç¨‹åºè‡ªåŠ¨ä¸‹è½½æ‰€éœ€çš„æ¨¡å‹æ–‡ä»¶ã€‚å›½å†…ç”¨æˆ·éœ€è¦ç¿»å¢™æ‰èƒ½æ­£å¸¸ä¸‹è½½ã€‚  
The program automatically downloads the required model files.

## Text Normalize
æ–‡æœ¬æ­£è§„åŒ–ã€‚  
ç§»é™¤å¤šä½™çš„å­—ç¬¦ï¼ŒçŸ«æ­£é”™è¯¯çš„æ ‡ç‚¹ç¬¦å·ã€‚  
æ–‡æœ¬æ­£è§„åŒ–åŒ…å«æ ‡ç‚¹ç¬¦å·æ­£è§„åŒ–æ¨¡å‹ã€‚ä½†æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡çš„æ­£è§„åŒ–æ–‡æœ¬è¯­æ–™ç”¨äºè®­ç»ƒã€‚è¯·è€ƒè™‘å°†enable_punctuation_normalizeå‚æ•°ç½®ä¸ºFalseï¼Œä»¥ç¦ç”¨æ ‡ç‚¹ç¬¦å·æ­£è§„åŒ–ã€‚

Text normalization processing removes redundant characters and corrects incorrect punctuation.  
Text normalization includes a punctuation normalization model. But there is not enough high-quality normalized text corpus available for training. Please consider setting the enable_punctuation_normalize parameter to False to disable punctuation normalization.
```python
from distilnlp import text_normalize

text_normalize('î—¥The project wasâ€¯started in 2007 by David Cournapeau as a Google Summer of Code projectï¼Œ \nand since then many volunteers have contributed.\nSee the About us page for a list of core contributors. ')
# got: 'The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us page for a list of core contributors.'

text_normalize('ğŸ˜‡54ã€‚ å¦‡å¥³ç½²æ¨å‡ºäº†å¢å¼ºå¦‡å¥³ç»æµæƒèƒ½çŸ¥è¯†ç½‘å…³(å‚é˜…www.empowerwomenã€‚org)ï¼Œå¸®åŠ©å„åˆ©ç›Šæ”¸å…³æ–¹å»ºç«‹è”ç³»å¹¶åˆ†äº«ç»éªŒå’Œä¸“é•¿ã€‚')
# got: '54. å¦‡å¥³ç½²æ¨å‡ºäº†å¢å¼ºå¦‡å¥³ç»æµæƒèƒ½çŸ¥è¯†ç½‘å…³ï¼ˆå‚é˜…www.empowerwomen.orgï¼‰ï¼Œå¸®åŠ©å„åˆ©ç›Šæ”¸å…³æ–¹å»ºç«‹è”ç³»å¹¶åˆ†äº«ç»éªŒå’Œä¸“é•¿ã€‚'

text_normalize([
    'This is an English sentenceã€‚',
    'ç™¾åº¦çš„ç½‘å€æ˜¯ httpï¼š//www.baiduã€‚com'
])
# got: [
#   'This is an English sentence.',
#   'ç™¾åº¦çš„ç½‘å€æ˜¯ http://www.baidu.com'
# ]
```

## Chinese Tokenize
ä¸­æ–‡åˆ†è¯
```python
from distilnlp import distilnlp
chinese_tokenize([
    '6. è®²ä¹ ç­çš„å‚åŠ è€…æ˜¯åœ¨å›½å®¶å’ŒåŒºåŸŸåº”æ€¥æœºæ„å’ŒæœåŠ¡éƒ¨é—¨çš„ç®¡ç†å²—ä½ä¸Šå·¥ä½œäº†è‹¥å¹²å¹´çš„ä¸“ä¸šäººå‘˜ã€‚', 
    'äººæƒæ˜¯æ‰€æœ‰äººä¸ç”Ÿä¿±æ¥çš„æƒåˆ©ï¼Œä¸åˆ†å›½ç±ã€æ€§åˆ«ã€å®—æ•™æˆ–ä»»ä½•å…¶ä»–èº«ä»½ã€‚'
])
# got: [
#   ['6', '.', ' ', 'è®²ä¹ ç­', 'çš„', 'å‚åŠ è€…', 'æ˜¯', 'åœ¨', 'å›½å®¶', 'å’Œ', 'åŒºåŸŸ', 'åº”æ€¥', 'æœºæ„', 'å’Œ', 'æœåŠ¡', 'éƒ¨é—¨', 'çš„', 'ç®¡ç†', 'å²—ä½', 'ä¸Š', 'å·¥ä½œ', 'äº†', 'è‹¥å¹²å¹´', 'çš„', 'ä¸“ä¸š', 'äººå‘˜', 'ã€‚'], 
#   ['äººæƒ', 'æ˜¯', 'æ‰€æœ‰', 'äºº', 'ä¸ç”Ÿ', 'ä¿±æ¥', 'çš„', 'æƒåˆ©', 'ï¼Œ', 'ä¸', 'åˆ†', 'å›½ç±', 'ã€', 'æ€§åˆ«', 'ã€', 'å®—æ•™', 'æˆ–', 'ä»»ä½•', 'å…¶ä»–', 'èº«ä»½', 'ã€‚'],
# ]
```